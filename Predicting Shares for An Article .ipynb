{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn import pipeline,ensemble\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/ML Data Sets/training_data.csv\")\n",
    "test = pd.read_csv(\"C:/ML Data Sets/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "     0. url:                          \tURL of the article\n",
    "     1. timedelta:                    \tDays between the article publication and the dataset acquisition\n",
    "     2. n_tokens_title:                \tNumber of words in the title\n",
    "     3. n_tokens_content:              \tNumber of words in the content\n",
    "     4. n_unique_tokens:               \tRate of unique words in the content\n",
    "     5. n_non_stop_words:              \tRate of non-stop words in the content\n",
    "     6. n_non_stop_unique_tokens:      \tRate of unique non-stop words in the content\n",
    "     7. num_hrefs:                 \t    Number of links\n",
    "     8. num_self_hrefs:                \tNumber of links to other articles published by the same website\n",
    "     9. num_imgs:                     \tNumber of images\n",
    "    10. num_videos:                    \tNumber of videos\n",
    "    11. average_token_length:          \tAverage length of the words in the content\n",
    "    12. num_keywords:                  \tNumber of keywords in the metadata\n",
    "    13. data_channel_is_lifestyle:     \tIs data channel 'Lifestyle'?\n",
    "    14. data_channel_is_entertainment: \tIs data channel 'Entertainment'?\n",
    "    15. data_channel_is_bus:           \tIs data channel 'Business'?\n",
    "    16. data_channel_is_socmed:        \tIs data channel 'Social Media'?\n",
    "    17. data_channel_is_tech:          \tIs data channel 'Tech'?\n",
    "    18. data_channel_is_world:         \tIs data channel 'World'?\n",
    "    19. kw_min_min:                    \tWorst keyword (min. shares)\n",
    "    20. kw_max_min:                    \tWorst keyword (max. shares)\n",
    "    21. kw_avg_min:                    \tWorst keyword (avg. shares)\n",
    "    22. kw_min_max:                    \tBest keyword (min. shares)\n",
    "    23. kw_max_max:                    \tBest keyword (max. shares)\n",
    "    24. kw_avg_max:                   \tBest keyword (avg. shares)\n",
    "    25. kw_min_avg:                    \tAvg. keyword (min. shares)\n",
    "    26. kw_max_avg:                    \tAvg. keyword (max. shares)\n",
    "    27. kw_avg_avg:                    \tAvg. keyword (avg. shares)\n",
    "    28. self_reference_min_shares:    \tMin. shares of referenced articles in the Website\n",
    "    29. self_reference_max_shares:     \tMax. shares of referenced articles in the Website\n",
    "    30. self_reference_avg_sharess:    \tAvg. shares of referenced articles in the Website\n",
    "    31. weekday_is_monday:             \tWas the article published on a Monday?\n",
    "    32. weekday_is_tuesday:            \tWas the article published on a Tuesday?\n",
    "    33. weekday_is_wednesday:          \tWas the article published on a Wednesday?\n",
    "    34. weekday_is_thursday:           \tWas the article published on a Thursday?\n",
    "    35. weekday_is_friday:             \tWas the article published on a Friday?\n",
    "    36. weekday_is_saturday:           \tWas the article published on a Saturday?\n",
    "    37. weekday_is_sunday:             \tWas the article published on a Sunday?\n",
    "    38. is_weekend:                    \tWas the article published on the weekend?\n",
    "    39. LDA_00:                        \tCloseness to LDA topic 0\n",
    "    40. LDA_01:                       \tCloseness to LDA topic 1\n",
    "    41. LDA_02:                        \tCloseness to LDA topic 2\n",
    "    42. LDA_03:                        \tCloseness to LDA topic 3\n",
    "    43. LDA_04:                        \tCloseness to LDA topic 4\n",
    "    44. global_subjectivity:           \tText subjectivity\n",
    "    45. global_sentiment_polarity:     \tText sentiment polarity\n",
    "    46. global_rate_positive_words:    \tRate of positive words in the content\n",
    "    47. global_rate_negative_words:    \tRate of negative words in the content\n",
    "    48. rate_positive_words:           \tRate of positive words among non-neutral tokens\n",
    "    49. rate_negative_words:           \tRate of negative words among non-neutral tokens\n",
    "    50. avg_positive_polarity:         \tAvg. polarity of positive words\n",
    "    51. min_positive_polarity:         \tMin. polarity of positive words\n",
    "    52. max_positive_polarity:         \tMax. polarity of positive words\n",
    "    53. avg_negative_polarity:         \tAvg. polarity of negative  words\n",
    "    54. min_negative_polarity:         \tMin. polarity of negative  words\n",
    "    55. max_negative_polarity:         \tMax. polarity of negative  words\n",
    "    56. title_subjectivity:            \tTitle subjectivity\n",
    "    57. title_sentiment_polarity:      \tTitle polarity\n",
    "    58. abs_title_subjectivity:        \tAbsolute subjectivity level\n",
    "    59. abs_title_sentiment_polarity:  \tAbsolute polarity level\n",
    "    60. shares:                        \tNumber of shares (target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df_null_idx = df[df.isnull().sum(axis = 1) > 0].index\n",
    "df.iloc[df_null_idx]\n",
    "\n",
    "# no null values in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the distribution of shares and finding any outliers\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.subplot(2,1,1)\n",
    "df.shares.plot.hist(bins = 5,density = True)\n",
    "plt.subplot(2,1,2)\n",
    "df.shares.plot.box(vert = False)\n",
    "\n",
    "# no outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n",
    "    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio\n",
    "    \n",
    "    \n",
    "def train_test_split(df,test_size,id_column,hash = hashlib.md5):\n",
    "    ids = df[id_column]\n",
    "    in_test_set = ids.apply(lambda id_:test_set_check(id_,test_size,hash))\n",
    "    return df.loc[~in_test_set],df.loc[in_test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,validation = train_test_split(df,0.3,\"url_id\")\n",
    "print(train.shape)\n",
    "print(validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the data set into dependent and independent parts \n",
    "X_train = train.iloc[:,:-1]\n",
    "X_test = validation.iloc[:,:-1]\n",
    "y_train = train[\"shares\"]\n",
    "y_test = validation[\"shares\"]\n",
    "print(\"X_train: \",X_train.shape)\n",
    "print(\"y_train:\",y_train.shape)\n",
    "print(\"X_test:\",X_test.shape)\n",
    "print(\"y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline.Pipeline([\n",
    "    (\"poly\",PolynomialFeatures(degree = 2)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"fit\",ensemble.GradientBoostingRegressor(random_state=1))\n",
    "])\n",
    "param_grid = {\n",
    "    \"fit__learning_rate\": [0.1, 0.01],\n",
    "    \"fit__alpha\": np.linspace(0.001, 0.999, 5),\n",
    "}\n",
    "gs = model_selection.GridSearchCV(cv=5,estimator = pipe,param_grid = param_grid,verbose = True,scoring = \"r2\",n_jobs = -1)\n",
    "gs.fit(X_train,y_train)\n",
    "print(\"best_params\",gs.best_params_,\"best_scores\",-gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
